{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f43e4ec8-bfe9-4705-915b-720aab89fad6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "957602ab-d637-4b88-a63b-92790a4af89f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Select questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "949c212f-dbb4-4a49-a25c-7af0bb311e40",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Recyclable and Low Fat Products"
    }
   },
   "outputs": [],
   "source": [
    "data = [['0', 'Y', 'N'], ['1', 'Y', 'Y'], ['2', 'N', 'Y'], ['3', 'Y', 'Y'], ['4', 'N', 'N']]\n",
    "products = pd.DataFrame(data, columns=['product_id', 'low_fats', 'recyclable']).astype({'product_id':'int64', 'low_fats':'category', 'recyclable':'category'})\n",
    "\n",
    "products = spark.createDataFrame(products)\n",
    "\n",
    "products_1 = products.filter((products['low_fats'] == 'Y') & (products['recyclable'] == 'Y')).select('product_id')\n",
    "\n",
    "\n",
    "products_1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "542f597e-22a5-425e-9c90-1e58b498e9a0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Find Customer Referee"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = [[1, 'Will', None], [2, 'Jane', None], [3, 'Alex', 2], [4, 'Bill', None], [5, 'Zack', 1], [6, 'Mark', 2]]\n",
    "customer = pd.DataFrame(data, columns=['id', 'name', 'referee_id']).astype({'id':'Int64', 'name':'object', 'referee_id':'Int64'})\n",
    "\n",
    "customer = spark.createDataFrame(customer)\n",
    "customer.display()\n",
    "customer.filter((customer['referee_id'] != 2) | (customer['referee_id'].isNull() == True)).select('name').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "678400b0-2c52-4960-b0cf-76625e7af484",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Big Countries"
    }
   },
   "outputs": [],
   "source": [
    "data = [['Afghanistan', 'Asia', 652230, 25500100, 20343000000], ['Albania', 'duropd', 28748, 2831741, 12960000000], ['Algdria', 'Africa', 2381741, 37100000, 188681000000], ['Andorra', 'duropd', 468, 78115, 3712000000], ['Angola', 'Africa', 1246700, 20609294, 100990000000]]\n",
    "\n",
    "world = pd.DataFrame(data, columns=['name', 'continent', 'area', 'population', 'gdp']).astype({'name':'object', 'continent':'object', 'area':'Int64', 'population':'Int64', 'gdp':'Int64'})\n",
    "\n",
    "spark_df = spark.createDataFrame(world)\n",
    "\n",
    "spark_df_1 = spark_df[(spark_df['area']>= 3000000 ) | (spark_df['population']>=25000000)]\n",
    "\n",
    "spark_df_2 = spark_df_1[['name', 'area', 'population']].dropDuplicates()\n",
    "\n",
    "spark_df_2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a4fa3b1-5ab7-4ae2-bfe7-d0dbbb21efbb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Article Views 1"
    }
   },
   "outputs": [],
   "source": [
    "data = [[1, 3, 5, '2019-08-01'], [1, 3, 6, '2019-08-02'], [2, 7, 7, '2019-08-01'], [2, 7, 6, '2019-08-02'], [4, 7, 1, '2019-07-22'], [3, 4, 4, '2019-07-21'], [3, 4, 4, '2019-07-21']]\n",
    "views = pd.DataFrame(data, columns=['article_id', 'author_id', 'viewer_id', 'view_date']).astype({'article_id':'Int64', 'author_id':'Int64', 'viewer_id':'Int64', 'view_date':'datetime64[ns]'})\n",
    "\n",
    "views1 = spark.createDataFrame(views)\n",
    "\n",
    "ans = views1[(views1['author_id'] == views1['viewer_id'])][['author_id']].dropDuplicates().orderBy('author_id')\n",
    "\n",
    "ans1 = ans.select(col('author_id').alias('id'))\n",
    "\n",
    "ans1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60321259-649f-4cdc-88d1-d2f8bf480d9e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Invalid Tweets"
    }
   },
   "outputs": [],
   "source": [
    "data = [[1, 'Vote for Biden'], [2, 'Let us make America great again!']]\n",
    "tweets = pd.DataFrame(data, columns=['tweet_id', 'content']).astype({'tweet_id':'Int64', 'content':'object'})\n",
    "\n",
    "tw = spark.createDataFrame(tweets)\n",
    "\n",
    "ans = tw.filter(length(tw['content']) > 15).select(tw['tweet_id'])\n",
    "ans.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bedd3f8-097b-4c20-944f-aa7e982c7cf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71b38ce1-6ce7-436f-ae27-e995ee720340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### # Basic Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21019518-6c9b-480c-af41-83aa668377b7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Replace Employee ID With The Unique Identifier"
    }
   },
   "outputs": [],
   "source": [
    "data = [[1, 'Alice'], [7, 'Bob'], [11, 'Meir'], [90, 'Winston'], [3, 'Jonathan']]\n",
    "employees = pd.DataFrame(data, columns=['id', 'name']).astype({'id':'int64', 'name':'object'})\n",
    "data = [[3, 1], [11, 2], [90, 3]]\n",
    "employee_uni = pd.DataFrame(data, columns=['id', 'unique_id']).astype({'id':'int64', 'unique_id':'int64'})\n",
    "\n",
    "employees = spark.createDataFrame(employees)\n",
    "employee_uni = spark.createDataFrame(employee_uni)\n",
    "\n",
    "employees_1 = employees.alias('a').join(employee_uni.alias('b'), employees['id'] == employee_uni['id'], 'left').select('b.unique_id','a.name')\n",
    "\n",
    "employees_1.display()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fb39472-1cb2-48f2-a316-a1ecabd9af0c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Product Sales Analysis I"
    }
   },
   "outputs": [],
   "source": [
    "data = [[1, 100, 2008, 10, 5000], [2, 100, 2009, 12, 5000], [7, 200, 2011, 15, 9000]]\n",
    "sales = pd.DataFrame(data, columns=['sale_id', 'product_id', 'year', 'quantity', 'price']).astype({'sale_id':'Int64', 'product_id':'Int64', 'year':'Int64', 'quantity':'Int64', 'price':'Int64'})\n",
    "data = [[100, 'Nokia'], [200, 'Apple'], [300, 'Samsung']]\n",
    "product = pd.DataFrame(data, columns=['product_id', 'product_name']).astype({'product_id':'Int64', 'product_name':'object'})\n",
    "\n",
    "\n",
    "sales = spark.createDataFrame(sales)\n",
    "product = spark.createDataFrame(product)\n",
    "\n",
    "sales_1 = sales.alias('a').join(product.alias('b'), sales['product_id'] == product['product_id'], 'left').select('b.product_name','a.year','a.price')\n",
    "sales_1.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25e099f5-9221-4f79-be31-58e0a63f0ea1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Customer Who Visited but Did Not Make Any Transactions"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = [[1, 23], [2, 9], [4, 30], [5, 54], [6, 96], [7, 54], [8, 54]]\n",
    "visits = pd.DataFrame(data, columns=['visit_id', 'customer_id']).astype({'visit_id':'Int64', 'customer_id':'Int64'})\n",
    "\n",
    "visits = spark.createDataFrame(visits)\n",
    "\n",
    "data = [[2, 5, 310], [3, 5, 300], [9, 5, 200], [12, 1, 910], [13, 2, 970]]\n",
    "transactions = pd.DataFrame(data, columns=['transaction_id', 'visit_id', 'amount']).astype({'transaction_id':'Int64', 'visit_id':'Int64', 'amount':'Int64'})\n",
    "\n",
    "transactions = spark.createDataFrame(transactions)\n",
    "\n",
    "visits_1 = visits.alias('a').join(transactions.alias('b'), visits['visit_id'] == transactions['visit_id'], 'left').select('a.*', 'b.transaction_id')\n",
    "visits_2 = visits_1.filter(visits_1['transaction_id'].isNull()).select('customer_id')\n",
    "visits_3 = visits_2.groupBy('customer_id').count()\n",
    "visits_3 = visits_3.orderBy(F.desc('count'))\n",
    "visits_3.display()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43d2ebac-52b7-449c-8cbd-0d55306fcc60",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Rising Temperature"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = [[1, '2015-01-01', 10], [2, '2015-01-02', 25], [3, '2015-01-03', 20], [4, '2015-01-04', 30]]\n",
    "weather = pd.DataFrame(data, columns=['id', 'recordDate', 'temperature']).astype({'id':'Int64', 'recordDate':'datetime64[ns]', 'temperature':'Int64'})\n",
    "\n",
    "weather = spark.createDataFrame(weather)\n",
    "\n",
    "weather_1 = weather.alias('a').join(weather.alias('b'))\n",
    "\n",
    "weather_2 = weather_1.filter((weather_1[\"a.id\"] != weather_1[\"b.id\"]) & (weather_1['b.recordDate']>weather_1['a.recordDate'] ) )\n",
    "                             \n",
    "weather_3 = weather_2.withColumn(\"date_diff\", F.datediff(weather_1['b.recordDate'], weather_1['a.recordDate']))\n",
    "\n",
    "weather_4 = weather_3.filter((weather_3['date_diff'] == 1) & (weather_2['b.temperature'] > weather_2['a.temperature'] ))\n",
    "                         \n",
    "\n",
    "weather_4.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb5daa1c-c7c7-4193-878d-5e9c5a431cc0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Average Time of Process per Machine"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from  pyspark.sql import functions as F\n",
    "\n",
    "data = [[0, 0, 'start', 0.712], [0, 0, 'end', 1.52], [0, 1, 'start', 3.14], [0, 1, 'end', 4.12], [1, 0, 'start', 0.55], [1, 0, 'end', 1.55], [1, 1, 'start', 0.43], [1, 1, 'end', 1.42], [2, 0, 'start', 4.1], [2, 0, 'end', 4.512], [2, 1, 'start', 2.5], [2, 1, 'end', 5]]\n",
    "activity = pd.DataFrame(data, columns=['machine_id', 'process_id', 'activity_type', 'timestamp']).astype({'machine_id':'Int64', 'process_id':'Int64', 'activity_type':'object', 'timestamp':'Float64'})\n",
    "activity = spark.createDataFrame(activity)\n",
    "\n",
    "a = activity.alias('a')\n",
    "b = activity.alias('b')\n",
    "\n",
    "\n",
    "activity_1 = a.join(b, ((a['a.machine_id'] == b['b.machine_id']) & (a['a.process_id'] == b['b.process_id'])), 'left')\n",
    "activity_2 = activity_1.filter((activity_1['a.activity_type'] != activity_1['b.activity_type']) & (activity_1['b.timestamp'] >= activity_1['a.timestamp']))\n",
    "activity_3 = activity_2.withColumn('time_diff', (activity_2['b.timestamp'] - activity_2['a.timestamp']))\n",
    "activity_4 = activity_3.groupBy(activity_3['a.machine_id']).agg(F.avg('time_diff'))\n",
    "activity_4.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d48ff9b1-bbbf-4f64-b553-8fcdc96586a0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Employee Bonus"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [[3, 'Brad', None, 4000], [1, 'John', 3, 1000], [2, 'Dan', 3, 2000], [4, 'Thomas', 3, 4000]]\n",
    "employee = pd.DataFrame(data, columns=['empId', 'name', 'supervisor', 'salary']).astype({'empId':'Int64', 'name':'object', 'supervisor':'Int64', 'salary':'Int64'})\n",
    "data = [[2, 500], [4, 2000]]\n",
    "bonus = pd.DataFrame(data, columns=['empId', 'bonus']).astype({'empId':'Int64', 'bonus':'Int64'})\n",
    "\n",
    "em = spark.createDataFrame(employee)\n",
    "bns = spark.createDataFrame(bonus)\n",
    "\n",
    "em = em.join(bns, on = 'empId', how = 'left')\n",
    "em = em.filter((em['bonus']<=1000) | (em['bonus'].isNull()))\n",
    "em.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "126404a2-103a-4baf-a69a-6819237eb626",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Students and Examinations"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = [[1, 'Alice'], [2, 'Bob'], [13, 'John'], [6, 'Alex']]\n",
    "students = pd.DataFrame(data, columns=['student_id', 'student_name']).astype({'student_id':'Int64', 'student_name':'object'})\n",
    "data = [['Math'], ['Physics'], ['Programming']]\n",
    "subjects = pd.DataFrame(data, columns=['subject_name']).astype({'subject_name':'object'})\n",
    "data = [[1, 'Math'], [1, 'Physics'], [1, 'Programming'], [2, 'Programming'], [1, 'Physics'], [1, 'Math'], [13, 'Math'], [13, 'Programming'], [13, 'Physics'], [2, 'Math'], [1, 'Math']]\n",
    "examinations = pd.DataFrame(data, columns=['student_id', 'subject_name']).astype({'student_id':'Int64', 'subject_name':'object'})\n",
    "\n",
    "students = spark.createDataFrame(students)\n",
    "subjects = spark.createDataFrame(subjects)\n",
    "exa = spark.createDataFrame(examinations)\n",
    "\n",
    "exa_1 = exa.groupBy(exa['student_id'],exa['subject_name']).count()\n",
    "exa_2 = exa_1.alias('a').join(students.alias('b'),exa_1['student_id'] == students['student_id'], 'left').select('a.*', 'b.student_name')\n",
    "\n",
    "students_1 = students.join(subjects)\n",
    "students_2 = students_1.alias('a').join(exa_2.alias('b'), (students_1['student_id'] == exa_2['student_id']) & (students_1['subject_name'] == exa_2['subject_name']),'left').select('a.*', 'b.count')\n",
    "\n",
    "students_3 = students_2.withColumn('new_count', F.when((students_2['count'].isNull()), 0).otherwise(students_2['count'])).select(students_2['student_id'], students_2['student_name'], students_2['subject_name'],'new_count')\n",
    "students_3.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "276afb66-d246-4aaa-a229-8c280faccd5c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Managers with at Least 5 Direct Reports"
    }
   },
   "outputs": [],
   "source": [
    "data = [[101, 'John', 'A', None], [102, 'Dan', 'A', 101], [103, 'James', 'A', 101], [104, 'Amy', 'A', 101], [105, 'Anne', 'A', 101], [106, 'Ron', 'B', 101]]\n",
    "employee = pd.DataFrame(data, columns=['id', 'name', 'department', 'managerId']).astype({'id':'Int64', 'name':'object', 'department':'object', 'managerId':'Int64'})\n",
    "\n",
    "employee = spark.createDataFrame(employee)\n",
    "\n",
    "manager_df = employee.filter(employee['managerId'].isNotNull()).groupBy('managerId').agg(F.count('id').alias('count')).withColumnRenamed('managerId','id')\n",
    "employee_ans = employee.join(manager_df, on = 'id', how = 'inner').select('name')\n",
    "employee_ans.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85f7d716-85d1-4fcd-9692-24ebeb99da8f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Confirmation Rate"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "data = [[3, '2020-03-21 10:16:13'], [7, '2020-01-04 13:57:59'], [2, '2020-07-29 23:09:44'], [6, '2020-12-09 10:39:37']]\n",
    "signups = pd.DataFrame(data, columns=['user_id', 'time_stamp']).astype({'user_id':'Int64', 'time_stamp':'datetime64[ns]'})\n",
    "data = [[3, '2021-01-06 03:30:46', 'timeout'], [3, '2021-07-14 14:00:00', 'timeout'], [7, '2021-06-12 11:57:29', 'confirmed'], [7, '2021-06-13 12:58:28', 'confirmed'], [7, '2021-06-14 13:59:27', 'confirmed'], [2, '2021-01-22 00:00:00', 'confirmed'], [2, '2021-02-28 23:59:59', 'timeout']]\n",
    "confirmations = pd.DataFrame(data, columns=['user_id', 'time_stamp', 'action']).astype({'user_id':'Int64', 'time_stamp':'datetime64[ns]', 'action':'object'})\n",
    "\n",
    "sig = spark.createDataFrame(signups)\n",
    "\n",
    "sig_o = sig\n",
    "cnf = spark.createDataFrame(confirmations)\n",
    "\n",
    "sig = sig.join(cnf, on = 'user_id', how = 'left')\n",
    "sig = sig.select(sig['user_id'],sig['action'])\n",
    "\n",
    "sig_cnf  = sig.filter(sig['action'] == 'confirmed')\n",
    "sig_cnf = sig_cnf.groupBy('user_id','action').count()\n",
    "sig_cnf = sig_cnf.withColumn('sig_cnf', sig_cnf['count'])\n",
    "\n",
    "sig_tim  = sig.filter(sig['action'] == 'timeout')\n",
    "sig_tim = sig_tim.groupBy('user_id','action').count()\n",
    "sig_tim = sig_tim.withColumn('sig_tim', sig_tim['count'])\n",
    "\n",
    "sig_1 = sig_o.join(sig_cnf, on = 'user_id', how = 'left')\n",
    "sig_1 = sig_1.join(sig_tim, on = 'user_id', how = 'left')\n",
    "sig_1 = sig_1.select('user_id','sig_cnf','sig_tim')\n",
    "sig_1 = sig_1.withColumn('act_f', F.when(sig_1['sig_cnf'].isNull(), sig_1['sig_tim'])\\\n",
    "                                   .when(sig_1['sig_cnf'].isNotNull() & sig_1['sig_tim'].isNotNull(), sig_1['sig_tim']+sig_1['sig_cnf'])\n",
    "                                   .otherwise(sig_1['sig_cnf']))\n",
    "# sig_1 = sig_1.select('user_id','act_f')\n",
    "sig_1 = sig_1.withColumn('cnf%', sig_1['sig_cnf']/sig_1['act_f'])\n",
    "sig_1 = sig_1.withColumn('ans', F.when(sig_1['cnf%'].isNull(), 0.00).otherwise(F.round(sig_1['cnf%'],2)))\n",
    "\n",
    "sig_1.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d04304e3-8dd7-407e-932b-257f9fd44c25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Basic Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37a7c851-b8f0-4961-b88d-c93f1b83732b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Not Boring Movies"
    }
   },
   "outputs": [],
   "source": [
    "data = [[1, 'War', 'great 3D', 8.9], [2, 'Science', 'fiction', 8.5], [3, 'irish', 'boring', 6.2], [4, 'Ice song', 'Fantacy', 8.6], [5, 'House card', 'Interesting', 9.1]]\n",
    "cinema = pd.DataFrame(data, columns=['id', 'movie', 'description', 'rating']).astype({'id':'Int64', 'movie':'object', 'description':'object', 'rating':'Float64'})\n",
    "\n",
    "cinema = spark.createDataFrame(cinema)\n",
    "\n",
    "cinema_1 = cinema.filter((cinema['id']%2 != 0) & (cinema['description'] != 'boring'))\n",
    "cinema_1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d603a0c-47e2-486a-91fe-6751db4b054f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Average Selling Price"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = [[1, '2019-02-17', '2019-02-28', 5], [1, '2019-03-01', '2019-03-22', 20], [2, '2019-02-01', '2019-02-20', 15], [2, '2019-02-21', '2019-03-31', 30]]\n",
    "prices = pd.DataFrame(data, columns=['product_id', 'start_date', 'end_date', 'price']).astype({'product_id':'Int64', 'start_date':'datetime64[ns]', 'end_date':'datetime64[ns]', 'price':'Int64'})\n",
    "data = [[1, '2019-02-25', 100], [1, '2019-03-01', 15], [2, '2019-02-10', 200], [2, '2019-03-22', 30]]\n",
    "units_sold = pd.DataFrame(data, columns=['product_id', 'purchase_date', 'units']).astype({'product_id':'Int64', 'purchase_date':'datetime64[ns]', 'units':'Int64'})\n",
    "\n",
    "pr = spark.createDataFrame(prices)\n",
    "us = spark.createDataFrame(units_sold)\n",
    "\n",
    "us_1 = us.join(pr, on = 'product_id',how = 'inner')\n",
    "\n",
    "us_2 = us_1.filter(us_1['purchase_date'].between(us_1['start_date'],us_1['end_date'])).withColumn('amount',us_1['price']*us_1['units'] )\n",
    "us_3 = us_2.groupBy('product_id').agg(F.sum('amount'), F.sum('units'))\n",
    "us_4 = us_3.withColumn('aavgg',F.round(us_3['sum(amount)']/us_3['sum(units)'],2))\n",
    "us_4.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0061cfa-e30c-48a6-a8f7-66536ad69718",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Project Employees I"
    }
   },
   "outputs": [],
   "source": [
    "data = [[1, 1], [1, 2], [1, 3], [2, 1], [2, 4]]\n",
    "project = pd.DataFrame(data, columns=['project_id', 'employee_id']).astype({'project_id':'Int64', 'employee_id':'Int64'})\n",
    "data = [[1, 'Khaled', 3], [2, 'Ali', 2], [3, 'John', 1], [4, 'Doe', 2]]\n",
    "employee = pd.DataFrame(data, columns=['employee_id', 'name', 'experience_years']).astype({'employee_id':'Int64', 'name':'object', 'experience_years':'Int64'})\n",
    "\n",
    "project = spark.createDataFrame(project)\n",
    "employee = spark.createDataFrame(employee)\n",
    "\n",
    "project_1 = project.join(employee, project['employee_id'] == employee['employee_id'], 'left').select('project_id', 'experience_years')\n",
    "\n",
    "project_2 = project_1.groupBy(project_1['project_id']).agg(F.avg(project_1['experience_years']))\n",
    "\n",
    "project_2.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ab8a768-ca0b-4d21-8bc7-2bd07d540c30",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Percentage of Users Attended a Contest"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = [[6, 'Alice'], [2, 'Bob'], [7, 'Alex']]\n",
    "users = pd.DataFrame(data, columns=['user_id', 'user_name']).astype({'user_id':'Int64', 'user_name':'object'})\n",
    "data = [[215, 6], [209, 2], [208, 2], [210, 6], [208, 6], [209, 7], [209, 6], [215, 7], [208, 7], [210, 2], [207, 2], [210, 7]]\n",
    "register = pd.DataFrame(data, columns=['contest_id', 'user_id']).astype({'contest_id':'Int64', 'user_id':'Int64'})\n",
    "\n",
    "users = spark.createDataFrame(users)\n",
    "register = spark.createDataFrame(register)\n",
    "\n",
    "temp = users.count()\n",
    "reg_1 = register.groupBy('contest_id').count()\n",
    "\n",
    "reg_2 = reg_1.withColumn('reg_per', F.round((reg_1['count']*100)/temp)).orderBy('contest_id').orderBy('reg_per', ascending  = False)\n",
    "\n",
    "\n",
    "\n",
    "reg_2.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cbe3eea-4bc7-46b0-8a3b-b9578d76654c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Queries Quality and Percentage"
    }
   },
   "outputs": [],
   "source": [
    "data = [['Dog', 'Golden Retriever', 1, 5], ['Dog', 'German Shepherd', 2, 5], ['Dog', 'Mule', 200, 1], ['Cat', 'Shirazi', 5, 2], ['Cat', 'Siamese', 3, 3], ['Cat', 'Sphynx', 7, 4]]\n",
    "queries = pd.DataFrame(data, columns=['query_name', 'result', 'position', 'rating']).astype({'query_name':'object', 'result':'object', 'position':'Int64', 'rating':'Int64'})\n",
    "\n",
    "q = spark.createDataFrame(queries)\n",
    "q = q.withColumn('pos', q['rating']/q['position']).withColumn('flag',F.when(q['rating'] < 3,1).otherwise(0))\n",
    "q_1 = q.groupBy('query_name').agg(\n",
    "  F.round(F.avg('pos'),2),\n",
    "  F.round(F.avg('flag')*100,2),\n",
    "  )\n",
    "\n",
    "q_1.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd1a926f-41ac-4aa6-a41b-907c1b098e5d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Monthly Transactions I"
    }
   },
   "outputs": [],
   "source": [
    "data = [[121, 'US', 'approved', 1000, '2018-12-18'], [122, 'US', 'declined', 2000, '2018-12-19'], [123, 'US', 'approved', 2000, '2019-01-01'], [124, 'DE', 'approved', 2000, '2019-01-07']]\n",
    "transactions = pd.DataFrame(data, columns=['id', 'country', 'state', 'amount', 'trans_date']).astype({'id':'Int64', 'country':'object', 'state':'object', 'amount':'Int64', 'trans_date':'datetime64[ns]'})\n",
    "\n",
    "txn = spark.createDataFrame(transactions)\n",
    "txn_1 = txn.withColumn('txn_date', F.date_format(txn['trans_date'], 'yyyy-MM')).select('txn_date','country','state','amount')\n",
    "\n",
    "txn_2a = txn_1.filter(txn_1['state'] == 'approved') \n",
    "txn_2a_1 = txn_2a.groupBy('txn_date', 'country').agg(F.count('*'), F.sum('amount'))\n",
    "\n",
    "\n",
    "txn_2 = txn_1.groupBy('txn_date', 'country').agg(F.count('*'), F.sum('amount'))\n",
    "\n",
    "txn_3 = txn_2.join(txn_2a_1, ((txn_2['txn_date'] == txn_2a_1['txn_date']) &(txn_2['country'] == txn_2a_1['country'])), 'left')\n",
    "\n",
    "txn_3.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9a017d6-5af5-49df-a960-289ef13748be",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Immediate Food Delivery II"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = [[1, 1, '2019-08-01', '2019-08-02'], [2, 2, '2019-08-02', '2019-08-02'], [3, 1, '2019-08-11', '2019-08-12'], [4, 3, '2019-08-24', '2019-08-24'], [5, 3, '2019-08-21', '2019-08-22'], [6, 2, '2019-08-11', '2019-08-13'], [7, 4, '2019-08-09', '2019-08-09']]\n",
    "delivery = pd.DataFrame(data, columns=['delivery_id', 'customer_id', 'order_date', 'customer_pref_delivery_date']).astype({'delivery_id':'Int64', 'customer_id':'Int64', 'order_date':'datetime64[ns]', 'customer_pref_delivery_date':'datetime64[ns]'})\n",
    "\n",
    "delh = spark.createDataFrame(delivery)\n",
    "\n",
    "window_spec = Window.partitionBy('customer_id').orderBy('order_date')\n",
    "\n",
    "delh = delh.withColumn('rnk', F.row_number().over(window_spec))\n",
    "delh = delh.withColumn('ans',F.when((delh['rnk'] == 1) & (delh['order_date'] == delh['customer_pref_delivery_date']), 1).otherwise(0))\n",
    "\n",
    "delh_ans = delh.groupBy('customer_id').agg(F.max('ans').alias('delivery_status'))\n",
    "ans = delh_ans.select(F.sum('delivery_status')/F.count('delivery_status'))\n",
    "\n",
    "delh.display()\n",
    "delh_ans.display()\n",
    "ans.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb9aaf0f-f479-4535-9d17-a18b6ff652d3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Game Play Analysis IV"
    }
   },
   "outputs": [],
   "source": [
    "data = [[1, 2, '2016-03-01', 5], [1, 2, '2016-03-02', 6], [2, 3, '2017-06-25', 1], [3, 1, '2016-03-02', 0], [3, 4, '2018-07-03', 5]]\n",
    "activity = pd.DataFrame(data, columns=['player_id', 'device_id', 'event_date', 'games_played']).astype({'player_id':'Int64', 'device_id':'Int64', 'event_date':'datetime64[ns]', 'games_played':'Int64'})\n",
    "\n",
    "activity = spark.createDataFrame(activity)\n",
    "window_spec = Window.partitionBy('player_id').orderBy('event_date')\n",
    "\n",
    "activity_1 = activity.withColumn('rnk', F.row_number().over(window_spec))\n",
    "activity_1 = activity_1.filter(activity_1['rnk'] <= 2)\n",
    "activity_1 = activity_1.select('player_id','event_date')\n",
    "\n",
    "activity_1_a = activity_1.withColumnRenamed('event_date', 'event_date_a')\n",
    "activity_1_b = activity_1.withColumnRenamed('event_date', 'event_date_b')\n",
    "\n",
    "activity_1 = activity_1_a.join(activity_1_b, on='player_id', how='inner')\n",
    "activity_1 = activity_1.filter(F.datediff('event_date_a','event_date_b') == 1)\n",
    "\n",
    "distinct_player_id = activity.select('player_id').dropDuplicates().count()\n",
    "\n",
    "total_player_id = activity_1.count()\n",
    "\n",
    "print(total_player_id/distinct_player_id)\n",
    "\n",
    "# activity_1.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "734db0ab-02c2-494c-936a-1855601314f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Sorting and Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9a5770f-6f1c-42a7-9e97-cb4b8c07f606",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Number of Unique Subjects Taught by Each Teacher"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = [[1, 2, 3], [1, 2, 4], [1, 3, 3], [2, 1, 1], [2, 2, 1], [2, 3, 1], [2, 4, 1]]\n",
    "teacher = pd.DataFrame(data, columns=['teacher_id', 'subject_id', 'dept_id']).astype({'teacher_id':'Int64', 'subject_id':'Int64', 'dept_id':'Int64'})\n",
    "\n",
    "tech = spark.createDataFrame(teacher)\n",
    "\n",
    "tech_1 = tech.select('teacher_id', 'subject_id').distinct()\n",
    "tech_2 = tech_1.groupBy('teacher_id').count()\n",
    "\n",
    "tech_2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d02af8c-9d9b-43f4-9e27-d66bf2616983",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "User Activity for the Past 30 Days I"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [[1, 1, '2019-07-20', 'open_session'], [1, 1, '2019-07-20', 'scroll_down'], [1, 1, '2019-07-20', 'end_session'], [2, 4, '2019-07-20', 'open_session'], [2, 4, '2019-07-21', 'send_message'], [2, 4, '2019-07-21', 'end_session'], [3, 2, '2019-07-21', 'open_session'], [3, 2, '2019-07-21', 'send_message'], [3, 2, '2019-07-21', 'end_session'], [4, 3, '2019-06-25', 'open_session'], [4, 3, '2019-06-25', 'end_session']]\n",
    "activity = pd.DataFrame(data, columns=['user_id', 'session_id', 'activity_date', 'activity_type']).astype({'user_id':'Int64', 'session_id':'Int64', 'activity_date':'datetime64[ns]', 'activity_type':'object'})\n",
    "\n",
    "activity = spark.createDataFrame(activity)\n",
    "\n",
    "activity_2 = activity.withColumn('date_diff', F.datediff(F.lit('2019-07-27'),activity['activity_date']))\n",
    "\n",
    "activity_3 = activity_2.filter(activity_2['date_diff'] < 30 ).select(activity_2['user_id'], activity_2['activity_date']).distinct()\n",
    "activity_4 = activity_3.groupBy(activity_3['activity_date']).count()\n",
    "\n",
    "activity_4.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a6deb55-f975-44bc-883a-42db59b12a1a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Product Sales Analysis III"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "\n",
    "data = [[1, 100, 2008, 10, 5000], [2, 100, 2009, 12, 5000], [7, 200, 2011, 15, 9000]]\n",
    "sales = pd.DataFrame(data, columns=['sale_id', 'product_id', 'year', 'quantity', 'price']).astype({'sale_id':'Int64', 'product_id':'Int64', 'year':'Int64', 'quantity':'Int64', 'price':'Int64'})\n",
    "data = [[100, 'Nokia'], [200, 'Apple'], [300, 'Samsung']]\n",
    "product = pd.DataFrame(data, columns=['product_id', 'product_name']).astype({'product_id':'Int64', 'product_name':'object'})\n",
    "\n",
    "sales = spark.createDataFrame(sales)\n",
    "\n",
    "window_spec = Window.partitionBy('product_id').orderBy('year')\n",
    "\n",
    "sales_1 = sales.withColumn('rnk', row_number().over(window_spec)).filter('rnk == 1')\n",
    "\n",
    "sales_1.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "849038e9-4844-42ee-b9d8-44398a693573",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Classes More Than 5 Students"
    }
   },
   "outputs": [],
   "source": [
    "data = [['A', 'Math'], ['B', 'English'], ['C', 'Math'], ['D', 'Biology'], ['E', 'Math'], ['F', 'Computer'], ['G', 'Math'], ['H', 'Math'], ['I', 'Math']]\n",
    "c = pd.DataFrame(data, columns=['student', 'class']).astype({'student':'object', 'class':'object'})\n",
    "\n",
    "c_1 = spark.createDataFrame(c)\n",
    "\n",
    "c_2 = c_1.select('class')\n",
    "c_3 = c_2.groupBy('class').count()\n",
    "c_4 = c_3.filter(c_3['count'] >= 5).select('class')\n",
    "c_4.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51f555fb-2ad1-4851-be02-20ff2c6f06a1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Find Follower Count"
    }
   },
   "outputs": [],
   "source": [
    "data = [['0', '1'], ['1', '0'], ['2', '0'], ['2', '1']]\n",
    "followers = pd.DataFrame(data, columns=['user_id', 'follower_id']).astype({'user_id':'Int64', 'follower_id':'Int64'})\n",
    "\n",
    "followers = spark.createDataFrame(followers)\n",
    "followers = followers.groupBy('user_id').agg(F.count('follower_id')).orderBy('user_id')\n",
    "followers.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5f5ef1f-b64c-4673-a9f5-c8549e5d6a57",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Biggest Single Number"
    }
   },
   "outputs": [],
   "source": [
    "data = [[8], [8], [3], [3], [1], [4], [5], [6]]\n",
    "my_numbers = pd.DataFrame(data, columns=['num']).astype({'num':'Int64'})\n",
    "\n",
    "my_numbers = spark.createDataFrame(my_numbers)\n",
    "my_numbers = my_numbers.groupBy('num').agg(F.count('num').alias('freq'))\n",
    "my_numbers = my_numbers.filter(my_numbers['freq'] == 1)\n",
    "my_numbers = my_numbers.agg(F.max(my_numbers['num']))\n",
    "\n",
    "my_numbers.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d617bfe-1085-40f2-9e01-81653d8f543d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Customers Who Bought All Products"
    }
   },
   "outputs": [],
   "source": [
    "data = [[1, 5], [2, 6], [3, 5], [3, 6], [1, 6]]\n",
    "customer = pd.DataFrame(data, columns=['customer_id', 'product_key']).astype({'customer_id':'Int64', 'product_key':'Int64'})\n",
    "data = [[5], [6]]\n",
    "product = pd.DataFrame(data, columns=['product_key']).astype({'product_key':'Int64'})\n",
    "\n",
    "cust =  spark.createDataFrame(customer)\n",
    "product =  spark.createDataFrame(product)\n",
    "\n",
    "window_spec = Window.partitionBy('customer_id','product_key').orderBy('product_key')\n",
    "\n",
    "cust = cust.withColumn('rnk', F.row_number().over(window_spec)).filter('rnk == 1')\n",
    "cust_1 = cust.groupBy('customer_id').agg(F.count('product_key').alias('cnt'))\n",
    "pr_cnt = product.dropDuplicates().count()\n",
    "cust_2 = cust_1.filter(cust_1['cnt'] == pr_cnt)\n",
    "cust_2.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7753feea-b3e4-4888-8f16-c86224764911",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Advanced Select and Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5246c815-b876-4339-896b-f454b5827a63",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "The Number of Employees Which Report to Each Employee"
    }
   },
   "outputs": [],
   "source": [
    "data = [[9, 'Hercy', None, 43], [6, 'Alice', 9, 41], [4, 'Bob', 9, 36], [2, 'Winston', None, 37]]\n",
    "employees = pd.DataFrame(data, columns=['employee_id', 'name', 'reports_to', 'age']).astype({'employee_id':'Int64', 'name':'object', 'reports_to':'Int64', 'age':'Int64'})\n",
    "\n",
    "emp = spark.createDataFrame(employees)\n",
    "\n",
    "emp_1 = emp.filter(emp['reports_to'].isNotNull())\n",
    "emp_1 = emp_1.groupBy(emp_1['reports_to']).agg(F.count(emp_1['name']).alias('reports_count'),F.round(F.avg(emp_1['age']),0).alias('average_age'))\n",
    "\n",
    "emp_2 = emp_1.join(emp, emp['employee_id'] == emp_1['reports_to'], 'left')\n",
    "emp_2 = emp_2.select('employee_id','name','reports_count','average_age')\n",
    "emp_2.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "777f6ea7-3dcc-45c2-9f43-b8eed8b61ea8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Primary Department for Each Employee"
    }
   },
   "outputs": [],
   "source": [
    "data = [['1', '1', 'N'], ['2', '1', 'Y'], ['2', '2', 'N'], ['3', '3', 'N'], ['4', '2', 'N'], ['4', '3', 'Y'], ['4', '4', 'N']]\n",
    "employee = pd.DataFrame(data, columns=['employee_id', 'department_id', 'primary_flag']).astype({'employee_id':'Int64', 'department_id':'Int64', 'primary_flag':'object'})\n",
    "\n",
    "emp = spark.createDataFrame(employee)\n",
    "\n",
    "window_spec = Window.partitionBy('employee_id').orderBy(emp['primary_flag'].desc())\n",
    "\n",
    "emp_1 = emp.withColumn('rnk', F.row_number().over(window_spec)).filter('rnk == 1').select('employee_id','department_id')\n",
    "emp_1.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c328017f-7e80-4243-8a5e-7fff08544421",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Triangle Judgement"
    }
   },
   "outputs": [],
   "source": [
    "data = [[13, 15, 30], [10, 20, 15]]\n",
    "triangle = pd.DataFrame(data, columns=['x', 'y', 'z']).astype({'x':'Int64', 'y':'Int64', 'z':'Int64'})\n",
    "\n",
    "tri = spark.createDataFrame(triangle)\n",
    "\n",
    "tri = tri.withColumn('fact_check', when((tri['x']+tri['y'] > tri['z']) & (tri['z']+tri['y'] > tri['x']) & ( tri['x']+tri['z'] > tri['y']) ,'Yes').otherwise('No'))\n",
    "\n",
    "tri.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b067531d-0c31-4f25-974f-d5f3696bab92",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Consecutive Numbers"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [[1, 1], [2, 1], [3, 1], [4, 2], [5, 1], [6, 2], [7, 2]]\n",
    "logs = pd.DataFrame(data, columns=['id', 'num']).astype({'id':'Int64', 'num':'Int64'})\n",
    "\n",
    "logs = spark.createDataFrame(logs)\n",
    "\n",
    "# logs_1 = logs.select('num').collect()\n",
    "\n",
    "# temp = logs_1[0]['num']\n",
    "# cnt = 0\n",
    "\n",
    "# lst = []\n",
    "# for rows in logs_1:\n",
    "#   if rows['num'] == temp:\n",
    "#     cnt+=1\n",
    "#     if cnt >= 3 and rows['num'] not in lst:\n",
    "#       lst.append(rows['num'])\n",
    "#       print(rows['num'])\n",
    "#   else :\n",
    "#     cnt = 1\n",
    "#     temp = rows['num']\n",
    "  \n",
    "logs_1 = logs.alias('a').join(logs.alias('b'), on ='num',how = 'left')\n",
    "logs_1 = logs_1.select('num',logs_1['a.id'].alias('a_id'),logs_1['b.id'].alias('b_id'))\n",
    "logs_1 = logs_1.alias('a').join(logs.alias('b'), on ='num',how = 'left')\n",
    "logs_1 = logs_1.select('num','a_id','b_id',logs_1['b.id'].alias('c_id'))\n",
    "logs_1 = logs_1.filter((logs_1['a_id']+1 == logs_1['b_id'])&(logs_1['b_id']+1 ==logs_1['c_id']))\n",
    "# logs_1 = logs.alias('c').join(logs.alias('d'), on ='num',how = 'left')\n",
    "\n",
    "logs.display()\n",
    "logs_1.display()\n",
    "logs_1.select(logs_1['num']).distinct().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b53d185-733d-4afd-88f4-e9ed3708a52a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Product Price at a Given Date"
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, '2019-08-15', 20),\n",
    "    (1, '2019-08-10', 15),\n",
    "    (2, '2019-08-16', 25),\n",
    "    (2, '2019-08-14', 20),\n",
    "    (3, '2019-08-12', None),\n",
    "]\n",
    "columns = ['product_id', 'change_date', 'new_price']\n",
    "\n",
    "products = spark.createDataFrame(data, schema=columns).withColumn(\"change_date\", F.to_date(\"change_date\"))\n",
    "\n",
    "window_spec = Window.partitionBy(\"product_id\").orderBy(F.col(\"change_date\").desc())\n",
    "filtered_products = products.filter(F.col(\"change_date\") <= \"2019-08-16\")\n",
    "ranked_products = filtered_products.withColumn(\"rnk\", F.row_number().over(window_spec))\n",
    "latest_products = ranked_products.filter(F.col(\"rnk\") == 1).select(\"product_id\", \"new_price\")\n",
    "final_df = products.select(\"product_id\").distinct() \\\n",
    "    .join(latest_products, on=\"product_id\", how=\"left\") \\\n",
    "    .withColumn(\"price\", F.when(F.col(\"new_price\").isNull(), F.lit(10)).otherwise(F.col(\"new_price\"))) \\\n",
    "    .select(\"product_id\", \"price\") \\\n",
    "    .orderBy(\"product_id\")\n",
    "\n",
    "final_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8552db48-c47a-4776-b1f0-ca8f87548025",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Last Person to Fit in the Bus"
    }
   },
   "outputs": [],
   "source": [
    "data = [[5, 'Alice', 250, 1], [4, 'Bob', 175, 5], [3, 'Alex', 350, 2], [6, 'John Cena', 400, 3], [1, 'Winston', 500, 6], [2, 'Marie', 200, 4]]\n",
    "queue = pd.DataFrame(data, columns=['person_id', 'person_name', 'weight', 'turn']).astype({'person_id':'Int64', 'person_name':'object', 'weight':'Int64', 'turn':'Int64'})\n",
    "\n",
    "queue = spark.createDataFrame(queue)\n",
    "\n",
    "window_spec = Window.orderBy('turn')\n",
    "\n",
    "queue_1 = queue.withColumn('cumulative_sum', F.sum('weight').over(window_spec)).filter('cumulative_sum <= 1000')\n",
    "queue_1 = queue_1.orderBy(queue_1['turn'].desc())\n",
    "queue_1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6650545-9055-4993-9de0-41510f52cd64",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Count Salary Categories"
    }
   },
   "outputs": [],
   "source": [
    "data = [[3, 108939], [2, 12747], [8, 87709], [6, 91796]]\n",
    "cat = ['Low Salary','Average Salary','High Salary']\n",
    "\n",
    "accounts = pd.DataFrame(data, columns=['account_id', 'income']).astype({'account_id':'Int64', 'income':'Int64'})\n",
    "cat = pd.DataFrame(cat,columns = ['cat'])\n",
    "\n",
    "acc = spark.createDataFrame(accounts)\n",
    "cat_df = spark.createDataFrame(cat)\n",
    "\n",
    "acc = acc.withColumn('cat', F.when(acc['income']<20000,'Low Salary')\n",
    "                             .when(acc['income']<=50000,'Average Salary')\n",
    "                             .otherwise('High Salary'))\n",
    "\n",
    "acc = acc.groupBy('cat').agg(F.count('account_id').alias('cnt'))\n",
    "\n",
    "new_df = cat_df.join(acc, on = 'cat', how = 'left')\n",
    "\n",
    "acc.display()\n",
    "new_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c26f97f-c378-40cf-af67-1cbd08b74984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Subqueries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "939745c9-46eb-4d79-bac3-ace786219caa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Employees Whose Manager Left the Company"
    }
   },
   "outputs": [],
   "source": [
    "data = [[3, 'Mila', 9, 60301], [12, 'Antonella', None, 31000], [13, 'Emery', None, 67084], [1, 'Kalel', 11, 21241], [9, 'Mikaela', None, 50937], [11, 'Joziah', 6, 28485]]\n",
    "employees = pd.DataFrame(data, columns=['employee_id', 'name', 'manager_id', 'salary']).astype({'employee_id':'Int64', 'name':'object', 'manager_id':'Int64', 'salary':'Int64'})\n",
    "\n",
    "emp = spark.createDataFrame(employees)\n",
    "\n",
    "emp_1 = emp.filter((emp['salary'] < 30000) & emp['manager_id'].isNotNull()).select('manager_id', 'employee_id')\n",
    "emp_2 = emp_1.join(emp, emp_1['manager_id'] == emp['employee_id'], 'left')\n",
    "emp_2.filter(emp_2['name'].isNull()).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bb6d84e-b432-4af3-a43e-59b9738cc1a9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Exchange Seats"
    }
   },
   "outputs": [],
   "source": [
    "data = [[1, 'Abbot'], [2, 'Doris'], [3, 'Emerson'], [4, 'Green'], [5, 'Jeames']]\n",
    "seat = pd.DataFrame(data, columns=['id', 'student']).astype({'id':'Int64', 'student':'object'})\n",
    "\n",
    "seat = spark.createDataFrame(seat)\n",
    "\n",
    "seat = seat.withColumn('new_col', F.round((seat['id']/2),0))\n",
    "\n",
    "window_spec = Window.partitionBy('new_col').orderBy('id')\n",
    "seat = seat.withColumn('lead_name', F.lead('student',1).over(window_spec))\\\n",
    "           .withColumn('lag_name', F.lag('student',1).over(window_spec))\n",
    "seat = seat.withColumn('final_student_name', when(seat['lead_name'].isNotNull(), seat['lead_name'])\n",
    "                                        .when(seat['lag_name'].isNotNull(), seat['lag_name'])\n",
    "                                        .otherwise(seat['student']))\n",
    "\n",
    "seat.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6fa12b0-6463-492a-832c-08b816ed7740",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Movie Rating"
    }
   },
   "outputs": [],
   "source": [
    "data = [[1, 'Avengers'], [2, 'Frozen 2'], [3, 'Joker']]\n",
    "movies = pd.DataFrame(data, columns=['movie_id', 'title']).astype({'movie_id':'Int64', 'title':'object'})\n",
    "data = [[1, 'Daniel'], [2, 'Monica'], [3, 'Maria'], [4, 'James']]\n",
    "users = pd.DataFrame(data, columns=['user_id', 'name']).astype({'user_id':'Int64', 'name':'object'})\n",
    "data = [[1, 1, 3, '2020-01-12'], [1, 2, 4, '2020-02-11'], [1, 3, 2, '2020-02-12'], [1, 4, 1, '2020-01-01'], [2, 1, 5, '2020-02-17'], [2, 2, 2, '2020-02-01'], [2, 3, 2, '2020-03-01'], [3, 1, 3, '2020-02-22'], [3, 2, 4, '2020-02-25']]\n",
    "movie_rating = pd.DataFrame(data, columns=['movie_id', 'user_id', 'rating', 'created_at']).astype({'movie_id':'Int64', 'user_id':'Int64', 'rating':'Int64', 'created_at':'datetime64[ns]'})\n",
    "\n",
    "movies = spark.createDataFrame(movies)\n",
    "users = spark.createDataFrame(users)\n",
    "movie_rating_o = spark.createDataFrame(movie_rating)\n",
    "\n",
    "movie_rating = movie_rating_o.groupBy('user_id').agg(F.count('movie_id').alias('cnt'))\n",
    "movie_rating = movie_rating.join(users, on = 'user_id', how = 'left')\n",
    "movie_rating = movie_rating.orderBy(movie_rating['cnt'].desc()).limit(1)\n",
    "movie_rating.display()\n",
    "\n",
    "movie_rating_n = movie_rating_o.filter((movie_rating_o['created_at']>=\"2020-02-01\") & (movie_rating_o['created_at']<=\"2020-02-29\"))\n",
    "movie_rating_n = movie_rating_n.groupBy('movie_id').agg(F.avg('rating').alias('avg_rating'))\n",
    "movie_rating_n = movie_rating_n.join(movies, on  = 'movie_id', how = 'left')\n",
    "movie_rating_n = movie_rating_n.orderBy(movie_rating_n['avg_rating'].desc(),movie_rating_n['title'])\n",
    "movie_rating_n.limit(1).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eef8d98e-d331-4086-806a-1a98729f6524",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restaurant Growth"
    }
   },
   "outputs": [],
   "source": [
    "data = [[1, 'Jhon', '2019-01-01', 100], [2, 'Daniel', '2019-01-02', 110], [3, 'Jade', '2019-01-03', 120], [4, 'Khaled', '2019-01-04', 130], [5, 'Winston', '2019-01-05', 110], [6, 'Elvis', '2019-01-06', 140], [7, 'Anna', '2019-01-07', 150], [8, 'Maria', '2019-01-08', 80], [9, 'Jaze', '2019-01-09', 110], [1, 'Jhon', '2019-01-10', 130], [3, 'Jade', '2019-01-10', 150]]\n",
    "customer = pd.DataFrame(data, columns=['customer_id', 'name', 'visited_on', 'amount']).astype({'customer_id':'Int64', 'name':'object', 'visited_on':'datetime64[ns]', 'amount':'Int64'})\n",
    "\n",
    "cust = spark.createDataFrame(customer)\n",
    "\n",
    "cust = cust.groupBy('visited_on').agg(F.sum('amount').alias('sales_sum'))\n",
    "# cust = cust_n.join(cust, on = 'visited_on', how = 'left')\n",
    "\n",
    "window_spec = Window.orderBy('visited_on')\n",
    "\n",
    "cust = cust.withColumn('last_1_day_sales', F.lag('sales_sum',1).over(window_spec))\\\n",
    "           .withColumn('last_2_day_sales', F.lag('sales_sum',2).over(window_spec))\\\n",
    "           .withColumn('last_3_day_sales', F.lag('sales_sum',3).over(window_spec))\\\n",
    "           .withColumn('last_4_day_sales', F.lag('sales_sum',4).over(window_spec))\\\n",
    "           .withColumn('last_5_day_sales', F.lag('sales_sum',5).over(window_spec))\\\n",
    "           .withColumn('last_6_day_sales', F.lag('sales_sum',6).over(window_spec))\n",
    "\n",
    "cust = cust.withColumn('required_sales', F.when(cust['last_6_day_sales'].isNotNull(), cust['last_1_day_sales']+cust['last_2_day_sales']+cust['last_3_day_sales']+cust['last_6_day_sales']+cust['last_5_day_sales']+cust['last_4_day_sales']+cust['sales_sum']).otherwise(None))\n",
    "\n",
    "cust = cust.withColumn('required_sales_avg', F.round(cust['required_sales']/7,2)).filter(cust['required_sales'].isNotNull()).select('visited_on','required_sales','required_sales_avg')                           \n",
    "\n",
    "cust.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "683f1010-6409-4e5f-a0a5-6196314d39d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Friend Requests II: Who Has the Most Friends"
    }
   },
   "outputs": [],
   "source": [
    "data = [[1, 2, '2016/06/03'], [1, 3, '2016/06/08'], [2, 3, '2016/06/08'], [3, 4, '2016/06/09']]\n",
    "request_accepted = pd.DataFrame(data, columns=['requester_id', 'accepter_id', 'accept_date']).astype({'requester_id':'Int64', 'accepter_id':'Int64', 'accept_date':'datetime64[ns]'})\n",
    "\n",
    "req = spark.createDataFrame(request_accepted)\n",
    "\n",
    "req_1 = req.groupBy('requester_id').agg(F.count('*').alias('cnt_1'))\n",
    "req_2 = req.groupBy('accepter_id').agg(F.count('*').alias('cnt_2'))\n",
    "\n",
    "req_f = req_1.join(req_2, req_1['requester_id'] == req_2['accepter_id'] , how = 'left')\n",
    "req_f = req_f.withColumn('total_friends', req_f['cnt_1']+req_f['cnt_2']).orderBy('total_friends', ascending = False)\n",
    "req_f = req_f.select(req_f['requester_id'].alias('id'), req_f['total_friends'].alias('num')).limit(1)\n",
    "req_f.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13790e63-28e3-4ad4-a015-c21807190ebd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Investments in 2016"
    }
   },
   "outputs": [],
   "source": [
    "data = [[1, 10, 5, 10, 10], [2, 20, 20, 20, 20], [3, 10, 30, 20, 20], [4, 10, 40, 40, 40]]\n",
    "insurance = pd.DataFrame(data, columns=['pid', 'tiv_2015', 'tiv_2016', 'lat', 'lon']).astype({'pid':'Int64', 'tiv_2015':'Float64', 'tiv_2016':'Float64', 'lat':'Float64', 'lon':'Float64'})\n",
    "\n",
    "ins = spark.createDataFrame(insurance)\n",
    "\n",
    "ins_1 = ins.groupBy('lat', 'lon').agg(F.count('*').alias('cnt'))\n",
    "ins_1 = ins_1.filter(ins_1['cnt'] == 1)\n",
    "\n",
    "ins_n = ins.join(ins_1, (ins['lat'] == ins_1['lat']) & (ins['lon'] == ins_1['lon']),'inner')\n",
    "ins_ans = ins_n.groupBy(ins_n['tiv_2015']).agg(F.sum('tiv_2016').alias('tiv_2016'))\n",
    "ins_ans.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61faf011-2b17-43a4-9f05-6d19c435431b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Department Top Three Salaries"
    }
   },
   "outputs": [],
   "source": [
    "data = [[1, 'Joe', 85000, 1], [2, 'Henry', 80000, 2], [3, 'Sam', 60000, 2], [4, 'Max', 90000, 1], [5, 'Janet', 69000, 1], [6, 'Randy', 85000, 1], [7, 'Will', 70000, 1]]\n",
    "employee = pd.DataFrame(data, columns=['id', 'name', 'salary', 'departmentId']).astype({'id':'Int64', 'name':'object', 'salary':'Int64', 'departmentId':'Int64'})\n",
    "data = [[1, 'IT'], [2, 'Sales']]\n",
    "department = pd.DataFrame(data, columns=['departmentId', 'name']).astype({'departmentId':'Int64', 'name':'object'})\n",
    "\n",
    "emp_o = spark.createDataFrame(employee)\n",
    "dep = spark.createDataFrame(department)\n",
    "\n",
    "window_spec = Window.partitionBy('departmentId','salary').orderBy(emp_o['salary'].desc())\n",
    "emp = emp_o.withColumn('rnk', F.row_number().over(window_spec)).filter('rnk = 1')\n",
    "\n",
    "window_spec_1 = Window.partitionBy('departmentId').orderBy(emp['salary'].desc())\n",
    "emp_1 = emp.withColumn('rnk', F.row_number().over(window_spec_1)).filter('rnk <= 3')\n",
    "\n",
    "emp_2 = emp_1.groupBy('departmentId').agg(F.min('salary').alias('min_top3_salary'))\n",
    "\n",
    "emp_3 = emp_o.join(emp_2, on = 'departmentId', how = 'left')\\\n",
    "             .join(dep, on = 'departmentId', how = 'left')\n",
    "             \n",
    "emp_3 = emp_3.filter(emp_3['salary'] >= emp_3['min_top3_salary'])\n",
    "\n",
    "emp_1.display()\n",
    "emp_3.display()\n",
    "# emp_ans.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6af49522-aec1-4e46-bb13-282396e300c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Advanced String Functions / Regex / Clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6eb08a9-16b9-4a85-b434-e96c6dc07fb3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fix Names in a Table"
    }
   },
   "outputs": [],
   "source": [
    "data = [[1, 'aLice'], [2, 'bOB']]\n",
    "users = pd.DataFrame(data, columns=['user_id', 'name']).astype({'user_id':'Int64', 'name':'object'})\n",
    "\n",
    "users = spark.createDataFrame(users)\n",
    "\n",
    "users = users.withColumn(\"name\",F.initcap(\"name\"))\n",
    "\n",
    "users = users.select(\"user_id\", \"name\").orderBy(\"user_id\")\n",
    "users.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6089974-d892-4ddf-bd23-8f1fff7960f6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Patients With a Condition"
    }
   },
   "outputs": [],
   "source": [
    "data = [[1, 'Daniel', 'YFEV COUGH'], [2, 'Alice', ''], [3, 'Bob', 'DIAB100 MYOP'], [4, 'George', 'ACNE DIAB100'], [5, 'Alain', 'DIAB201']]\n",
    "patients = pd.DataFrame(data, columns=['patient_id', 'patient_name', 'conditions']).astype({'patient_id':'int64', 'patient_name':'object', 'conditions':'object'})\n",
    "\n",
    "patients = spark.createDataFrame(patients)\n",
    "\n",
    "patients = patients.filter(\n",
    "    (F.col(\"CONDITIONS\").like(\"DIAB1%\")) | \n",
    "    (F.col(\"CONDITIONS\").like(\"% DIAB1%\"))\n",
    ")\n",
    "patients.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deed925b-f2df-4c26-8cf5-3cc049327b03",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Delete Duplicate Emails"
    }
   },
   "outputs": [],
   "source": [
    "data = [[1, 'john@example.com'], [2, 'bob@example.com'], [3, 'john@example.com']]\n",
    "person = pd.DataFrame(data, columns=['id', 'email']).astype({'id':'int64', 'email':'object'})\n",
    "\n",
    "df = spark.createDataFrame(person)\n",
    "window_spec = Window.partitionBy('email').orderBy(df['id'])\n",
    "\n",
    "df = df.withColumn('rnk', F.row_number().over(window_spec)).filter('rnk == 1')\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c292b13-d6c0-4aff-9693-f2dd0d9b8517",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Second Highest Salary"
    }
   },
   "outputs": [],
   "source": [
    "data = [[1, 100], [2, 200], [3, 300]]\n",
    "employee = pd.DataFrame(data, columns=['id', 'salary']).astype({'id':'int64', 'salary':'int64'})\n",
    "\n",
    "employee = spark.createDataFrame(employee)\n",
    "\n",
    "distinct_salaries = employee.select(\"salary\").distinct().orderBy(F.desc(\"salary\"))\n",
    "\n",
    "second_highest_salary = distinct_salaries.limit(2).collect()\n",
    "\n",
    "print(second_highest_salary[1]['salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b41870c4-e629-4ed6-ac6a-55f5d8186bbe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Group Sold Products By The Date"
    }
   },
   "outputs": [],
   "source": [
    "data = [['2020-05-30', 'Headphone'], ['2020-06-01', 'Pencil'], ['2020-06-02', 'Mask'], ['2020-05-30', 'Basketball'], ['2020-06-01', 'Bible'], ['2020-06-02', 'Mask'], ['2020-05-30', 'T-Shirt']]\n",
    "activities = pd.DataFrame(data, columns=['sell_date', 'product']).astype({'sell_date':'datetime64[ns]', 'product':'object'})\n",
    "\n",
    "act_o = spark.createDataFrame(activities)\n",
    "\n",
    "act = act_o.groupBy('sell_date').agg(F.countDistinct('product').alias('cnt'), F.collect_list('product').alias('product_list'))\\\n",
    "           .withColumn('products',F.expr(\"array_join(sort_array(product_list), ',')\"))\n",
    "act.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "216edcd5-a0c9-42a2-8e80-8e3e0b303b4f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "List the Products Ordered in a Period"
    }
   },
   "outputs": [],
   "source": [
    "data = [[1, 'Leetcode Solutions', 'Book'], [2, 'Jewels of Stringology', 'Book'], [3, 'HP', 'Laptop'], [4, 'Lenovo', 'Laptop'], [5, 'Leetcode Kit', 'T-shirt']]\n",
    "products = pd.DataFrame(data, columns=['product_id', 'product_name', 'product_category']).astype({'product_id':'Int64', 'product_name':'object', 'product_category':'object'})\n",
    "\n",
    "data = [[1, '2020-02-05', 60], [1, '2020-02-10', 70], [2, '2020-01-18', 30], [2, '2020-02-11', 80], [3, '2020-02-17', 2], [3, '2020-02-24', 3], [4, '2020-03-01', 20], [4, '2020-03-04', 30], [4, '2020-03-04', 60], [5, '2020-02-25', 50], [5, '2020-02-27', 50], [5, '2020-03-01', 50]]\n",
    "orders = pd.DataFrame(data, columns=['product_id', 'order_date', 'unit']).astype({'product_id':'Int64', 'order_date':'datetime64[ns]', 'unit':'Int64'})\n",
    "\n",
    "products = spark.createDataFrame(products)\n",
    "orders = spark.createDataFrame(orders)\n",
    "\n",
    "\n",
    "orders_filtered = orders.filter((F.col(\"order_date\") >= \"2020-02-01\") & (F.col(\"order_date\") <= \"2020-02-29\"))\n",
    "\n",
    "orders_aggregated = (orders_filtered.groupBy(\"product_id\").agg(F.sum(\"unit\").alias(\"unit\")).filter(F.col(\"unit\") >= 100))\n",
    "\n",
    "result = (orders_aggregated.join(products, on=\"product_id\", how=\"left\").select(F.col(\"product_name\"), F.col(\"unit\")))\n",
    "\n",
    "result.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39e094c6-df24-4b02-bb75-c58f56565e84",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Find Users With Valid E-Mails"
    }
   },
   "outputs": [],
   "source": [
    "data = [[1, 'Winston', 'winston@leetcode.com'], [2, 'Jonathan', 'jonathanisgreat'], [3, 'Annabelle', 'bella-@leetcode.com'], [4, 'Sally', 'sally.come@leetcode.com'], [5, 'Marwan', 'quarz#2020@leetcode.com'], [6, 'David', 'david69@gmail.com'], [7, 'Shapiro', '.shapo@leetcode.com']]\n",
    "users = pd.DataFrame(data, columns=['user_id', 'name', 'mail']).astype({'user_id':'int64', 'name':'object', 'mail':'object'})\n",
    "\n",
    "valid_emails = spark.createDataFrame(users)\n",
    "\n",
    "\n",
    "valid_email_regex = r'^[A-Za-z][A-Za-z0-9_\\.\\-]*@leetcode\\.com$'\n",
    "valid_emails = valid_emails.filter((F.col(\"mail\").rlike(valid_email_regex)))\n",
    "\n",
    "valid_emails.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1160422138710173,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Leetcode_problems",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}